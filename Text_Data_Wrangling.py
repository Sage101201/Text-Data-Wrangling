# -*- coding: utf-8 -*-
"""task2_125.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10U9SKbprAjTO8OQqyvWF4HYeVOEMeC_G

<div class="alert alert-block alert-danger">

# FIT5196 Task 2 in Assessment 1
    
#### Student Name: Anish S, Michael Bradtke
#### Student ID: 34113339, 32492464

Date: 28/03/2024

Environment: Python

Libraries used:
* pandas (for extracting data from excel and handling it)
* nltk (for text processing the data - tokenization, stemming, finding n-grams etc.)
* collections (for aid in counting)
* re (for to simplify and aid in pattern searching)
* langdetect (for detecting the language used in comments/snippets)
* sklearn (for counting unigram and bigram token-frequencies in english comments)


**TASK 2 PART DONE BY ANISH S**
    </div>

<div class="alert alert-block alert-info">
    
## Table of Contents

</div>

[1. Introduction](#Intro) <br>
[2. Importing Libraries](#libs) <br>
[3. Examining Input File & Loading Data](#step1) <br>
[4. Text Exraction, cleaning and Processing](#step2&3) <br>
[5. Generating Vocab](#step4) <br>
$\;\;\;\;$[5.1. Vocabulary List](#write-vocab) <br>
[6. Sparse Matrix & CountVec](#step6) <br>
[7. Summary](#summary) <br>
[8. References](#Ref) <br>

<div class="alert alert-block alert-success">
    
## Introduction  <a class="anchor" name="Intro"></a>

This assessment concerns textual data and the aim is to extract data, process them, and transform them into a proper format.

The dataset provided consists of excel file with 30 sheets of data. Each of these sheets consists of a table of 2 columns - 'id' and 'snippet'.

The objective is to extract *English* comments from ***textOriginal*** fields from the 'snippet' column, only from those channels (identified by *channelId*) that have atleast 15 english comments to it, and then to generate a vocabulary and count-frequency of tokens from these comments.

There are 3 output files desired -
* .csv file containing the (*channel_id, all_comment_count, eng_comment_count*)
* vocab.txt file with the vocabulary of tokens (unigram and bigram) in the format (*token : token_index*)
* countvec.txt with the count-frquency of all words from vocab being present in the english comments of channels with the format (*channelId, token1:token1_frq, token2:token2_frq, .....*)

<div class="alert alert-block alert-success">
    
## Importing Libraries  <a class="anchor" name="libs"></a>

In this assessment, any python packages is permitted to be used. The following packages were used to accomplish the related tasks:

* **os:** to interact with the operating system, e.g. navigate through folders to read files - *to be used by evaluators at their discretion*
* **re:** to define and use regular expressions; to fasten the process of pattern matching.
* **pandas:** to work with dataframes; extract data to them and handle it.
* **langdetect:** to detect languges from of the raw-text.
* **collections:** to simplify the process of counting; setting counters to default values during intitialisation.
* **nltk:** text processing raw textual data - tokenisation, stemming, finding n-grams, etc.
* **sklearn:** counting token-frequencies using CountVectorizer

RUN THIS CELL ONCE AT THE START ðŸ‘‡
"""

!pip install langdetect

import os # have not used this but left for evaluators to make use of if needed during file upload and access
import re
import pandas as pd
from langdetect import DetectorFactory, detect
from collections import defaultdict, Counter
import csv
from nltk.tokenize import RegexpTokenizer
from nltk.stem import PorterStemmer
import nltk
from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder
from nltk.tokenize import MWETokenizer
import numpy as np
from nltk.util import ngrams
from nltk.probability import *
from sklearn.feature_extraction.text import CountVectorizer

"""-------------------------------------

<div class="alert alert-block alert-success">
    
## STEP 1 : Examining Input File & Loading Data into right format <a class="anchor" name="step1"></a>

MOUNTING MY DRIVE
"""

from google.colab import drive
drive.mount('/content/drive')

# Read the Excel file
excel_file = pd.ExcelFile('/content/drive/Shareddrives/FIT5196_S1_2024/A1/Students data/Task 2/Group125.xlsx')
excel_file

"""### Checking the structure data in input file"""

# show the names of all the sheets
excel_file.sheet_names

check_df = excel_file.parse('Sheet1')
check_df.head(103)

"""### Extracting the data in sheets to pandas dataframe"""

# Dictionary to store DataFrames for each sheet
dfs = {}

# Iterate through each sheet
for sheet_num in range(30):
    # Read the sheet
    sheet_name = f'Sheet{sheet_num}'
    df = pd.read_excel(excel_file, sheet_name=sheet_name)

    # Find the starting row and column of the table
    start_row, start_col = 0, 0
    for i in range(df.shape[0]):
        if not df.iloc[i].isnull().all():
            start_row = i
            break
    for j in range(df.shape[1]):
        if not df.iloc[:, j].isnull().all():
            start_col = j
            break

    # Read the table data into a DataFrame
    df = pd.read_excel(excel_file, sheet_name=sheet_name, header=None)
    df = df.iloc[start_row:, start_col:]

    # Set column names
    df.columns = df.iloc[0].fillna('Unnamed')
    df = df.iloc[2:]
    df.columns = ['id', 'snippet']  # Set column names to 'id' and 'snippet'

    # Reset index
    df.reset_index(drop=True, inplace=True)

    # Save the DataFrame to the dictionary
    dfs[sheet_name] = df

# Now dfs contains DataFrames for each sheet
# You can access them using dfs['Sheet0'], dfs['Sheet1'], etc.

dfs['Sheet1'].head(100)

dfs['Sheet29']

"""How many rows of data is to be there finally?"""

total_with_dup = 0
for sheet_name, df in dfs.items():
    print(f"Number of rows in {sheet_name}: {len(df)}")
    total_with_dup += len(df)

print(f"TOTAL ROWS : {total_with_dup}")

"""### Combining Data from all sheets and dropping duplicates"""

# Combine all DataFrames into one
combined_df = pd.concat(dfs.values(), ignore_index=True)

# Remove duplicates
combined_df.drop_duplicates(inplace=True)

# Reset index
combined_df.reset_index(drop=True, inplace=True)

print(len(combined_df))

combined_df

##### Step 1 COMPLETE

"""<div class="alert alert-block alert-success">
    
## STEP 2 & 3 : Text Exraction, cleaning and Processing <a class="anchor" name="step2&3"></a>

### STEP 2: Emoji Cleaning
"""

# Set the seed for langdetect
DetectorFactory.seed = 0

"""Extracting the list of emojis to remove from emoji.txt â–¶"""

# Load emoji list
with open('/content/drive/Shareddrives/FIT5196_S1_2024/A1/emoji.txt', 'r', encoding='utf-8') as file:
    emoji_list = file.read().splitlines()

print(emoji_list)

# Function to remove emojis from text
def remove_emojis(text, emoji_list):
    emoji_pattern = "|".join(map(re.escape, emoji_list))
    return re.sub(emoji_pattern, '', text)

"""#### Cleaning Emojis and extracting english comments and respective channelIds

**LOGIC:**
(for each 'snippet' row)

â†ª extract *textOriginal* fields from snippet.

$\;\;\;\;$JSON DATA STRUCTURE FORMAT -

$\;\;\;\;${......*'topLevelComment'* : {..... *'snippet'* : {...... *'channelId'*: {**DESIRED_CHANNEL**}, ..... *'textOriginal'*: {**DESIRED_COMMENT**}}}}

â†ª extract corresponding *channelId* fields from snippet as above. ðŸ‘†

â†ª remove emojis idenitifed in emoji_list.

â†ª filter out comments that are detectd to be of English language

â†ª add filtered comments and respective channelIds into a new dataframe
"""

# emoji - cleaning

# List to store channelId and English comments
english_comments_list = []

# Dictionary to store 'all_count' and 'eng_count' for each channel
channel_info = {}

# Process each channel's comments
for index, row in combined_df.iterrows():
    # Extract textOriginal from snippet
    snippet = eval(row['snippet'])
    top_level_comments = snippet.get('topLevelComment', {}).get('snippet', {})
    text_original = top_level_comments.get('textOriginal', '')

    # Extract channelId
    channel_id = snippet.get('topLevelComment', {}).get('snippet', {}).get('channelId', None)

    # Update channel_info dictionary
    if channel_id not in channel_info.keys():
        channel_info[channel_id] = {'all_count': 0, 'eng_count': 0}
    channel_info[channel_id]['all_count'] += 1

    # Remove emojis
    cleaned_comment = remove_emojis(text_original, emoji_list)

    # Language detection
    try:
        lang = detect(cleaned_comment)
        if lang == 'en':
            english_comments_list.append({'channelId': channel_id, 'comment': cleaned_comment.lower()})
            channel_info[channel_id]['eng_count'] += 1
    except:
        pass

# Create DataFrame for English comments
english_comments_df = pd.DataFrame(english_comments_list)

english_comments_df.head(100)

english_comments_df

"""NOTE:
- There are ~55k english comments extracted from the input file and cleaned for emojis

---

<br>
<br>

**âœ” CHECKPOINT #1 (for checking data)**
"""

# Specify the path where you want to save the .txt file
file_path = 'check.txt'

# Write the DataFrame to a .txt file
english_comments_df.to_csv(file_path, sep='\t', index=False)

final_count = 0
for key, value in channel_info.items():
    if value['eng_count'] > 14:
      final_count += value['eng_count']
print("FINAL COUNT:", final_count)

"""NOTE:
- There are more than ~51k english comments in total from channels with atleast 15 english comments


---
<br>
<br>

### STEP 3 - Outputting to .csv format (**Output File #1**)

We are finding the number of english comments and all comments made for each of the different channels from the input file
"""

# Define the CSV file path
csv_file = '125_channel_list.csv'

# Define the headers for the CSV file
headers = ['channel_id', 'all_comment_count', 'eng_comment_count']

# Write dictionary to CSV file
with open(csv_file, 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(headers)
    for channel_id, counts in channel_info.items():
        writer.writerow([channel_id, counts['all_count'], counts['eng_count']])

##### Step 2 & 3 COMPLETE!

"""<div class="alert alert-block alert-warning">
    
## STEP 4 : Generating Vocab - Unigram and Bigram Lists <a class="anchor" name="step4"></a>

### Generate Unigram Tokenlist

**LOGIC:**

â†ª 1. Filter out english comments made in channels that have more than 15 english comments.

â†ª 2. Word Tokenize the english comments using RegexTokenizer.

â†ª 3. Clear context independent stopwords from the word tokens.

â†ª 4. Stem the resultant token list using Porter Stemmer.

â†ª 5. Clear context dependent stopwords from the stemmed tokens. These are stopwords that appear in more than 99% of the channels.

â†ª 6. Remove rare tokens from the resultant stopwords removed, stemmed token lists. These are tokens that appear in less than 1% of the channels.

â†ª 7. Remove tokens with size < 3 from the resultant stopwords to generate final versions of unigram token list.



---

These are my **reasonings** to choose the above order of token processing:
1. I choose to remove context independent stopwords before the context dependent ones so that words like 'is', 'are', etc. which are really context-independent are differentiated from words that don't belong in the *stopwords_en.txt* but appear unnaturally frequently in the corpus of english comments (though I didn't get any when checked).
2. I chose to add the stemming before removal context dependent stopwords so as to increase the frequency of some words (in their base or stemmed forms) and accurately classify them as context dependent stopwords in the next step. Though in the case of rare tokens, the number of such tokens is reduced by stemming beforehand, it does lead to better vocabulary formation.
3. I could have chosen to remove tokens with size < 3 before stemming but that would result in removal of some very common context independent stopwords such as 'a', 'is', 'he', etc, which is already being taken care of in step 3 above; also stemmed tokens with length less than 2, especially after having removed stopwords, are words that don't particularly mean anything. This is why I chose to apply this step at last.
"""

# Step 1: Filter out Channels with more than 15 English comments
filtered_channels = [channel_id for channel_id, counts in channel_info.items() if counts['eng_count'] > 14]
filtered_comments_df = english_comments_df[english_comments_df['channelId'].isin(filtered_channels)].reset_index(drop=True)

filtered_comments_df.head(100)

print(filtered_comments_df['channelId'].nunique())

"""NOTE:
- there are 1315 unique channels that have atleast 15 english comments


---


"""

# Step 2: Word tokenize using RegexpTokenizer
tokenizer = RegexpTokenizer(r"[a-zA-Z]+")
filtered_comments_df['tokens'] = filtered_comments_df['comment'].apply(lambda x: tokenizer.tokenize(x.lower()))

filtered_comments_df.head(100)

"""**âœ” CHECKPOINT #2 (to check data generated)**"""

file_df = 'check2.txt'

filtered_comments_df.to_csv(file_df, sep='\t', index=False)

"""***Function to count no. of channels the token appears in***"""

# For removing context-dependent stopwords and rare tokens

def TTF(df_column):
  # Initialize a defaultdict to store the set of channels each token appears in
  token_channels = defaultdict(set)

  # Update the set of channels for each token
  for index, row in filtered_comments_df.iterrows():
      channel_id = row['channelId']
      tokens = row[df_column]
      for token in tokens:
          token_channels[token].add(channel_id)

  # Count the number of channels each token appears in
  total_token_freq = Counter()

  # Count the number of channels each token appears in
  for token, channels in token_channels.items():
        total_token_freq[token] = len(channels)

  return total_token_freq

# Identify context-dependent stopwords (words appearing in more than 99% of channels)
total_channels = len(filtered_channels)

# Step 3: Remove Context-independent stopwords
with open('/content/drive/Shareddrives/FIT5196_S1_2024/A1/stopwords_en.txt', 'r') as f:
    context_independent_stopwords = set(f.read().splitlines())

filtered_comments_df['cntInd-stopwords_removed_tokens'] = filtered_comments_df['tokens'].apply(lambda x: [token for token in x if token not in context_independent_stopwords])

filtered_comments_df.head(100)

# Step 4: Stemmatize the words using Porter Stemmer
porter_stemmer = PorterStemmer()
filtered_comments_df['cntInd-SW_removed_stemmed_tokens'] = filtered_comments_df['cntInd-stopwords_removed_tokens'].apply(lambda x: [porter_stemmer.stem(token) for token in x])

filtered_comments_df.head(100)

print(TTF('cntInd-SW_removed_stemmed_tokens'))

print(total_channels)

# Step 5: Remove Context-dependent stopwords
ttf_context_dep_stopwords = TTF('cntInd-SW_removed_stemmed_tokens')
context_dependent_stopwords = set(word for word, freq in ttf_context_dep_stopwords.items() if freq / total_channels > 0.99)

print(context_dependent_stopwords) # none

filtered_comments_df['all_stopwords_removed_stemmed_tokens'] = filtered_comments_df['cntInd-SW_removed_stemmed_tokens'].apply(lambda x: [token for token in x if token not in context_dependent_stopwords])

filtered_comments_df.head(100)

print(TTF('all_stopwords_removed_stemmed_tokens'))

# Step 6: Remove rare tokens from the vocab

ttf_rare_tokens = TTF('all_stopwords_removed_stemmed_tokens')
rare_tokens = set(token for token, freq in ttf_rare_tokens.items() if freq / total_channels < 0.01)

print(rare_tokens)

filtered_comments_df['SW-rem_stemmed_RT_removed_tokens'] = filtered_comments_df['all_stopwords_removed_stemmed_tokens'].apply(lambda x: [token for token in x if token not in rare_tokens])

filtered_comments_df.head(100)

# Step 7: Remove tokens with a length less than 3
filtered_comments_df['final_unigram_tokens'] = filtered_comments_df['SW-rem_stemmed_RT_removed_tokens'].apply(lambda x: [token for token in x if len(token) >= 3])

filtered_comments_df.head(100)

"""**âœ” CHECKPOINT #3 (for checking the final unigram tokenlists)**"""

all_tokenfile_df = 'check3.txt'

filtered_comments_df.to_csv(all_tokenfile_df, sep='\t', index=False)

"""### Generate top 200 meaningful Bigram Tokens

The meaningfullness of the bigrams from the word tokens is measured by PMI (Pointwise Mutual Information). It measures the ratio of observed collocationness of two words (appearing next to each other) compared to expected collocationness of these two words if they were just some twor randomly picked words.

The bigrams are generated from the list of word tokens obtained after applying word tokenization on english comments.

ðŸŒŸ The top 200 bigrams with the highest PMI are picked after ranking them in descending order after checking if they actually have atleast 10 collocations in token list (this is not their real token-frequencies). Actual token-frequencies are calculated wrt comments

*(I have done this so as to pick only those bigrams that are so common that they deserve to be added to the vocabulary and fasten up collocation checking.)*
"""

# Calculate PMI
bigram_measures = BigramAssocMeasures()
finder = BigramCollocationFinder.from_documents(filtered_comments_df['tokens'])
finder.apply_freq_filter(10) # atleast 10 collocations in token list
pmi_scores = finder.score_ngrams(bigram_measures.pmi)

# Sort bigrams based on PMI scores
sorted_bigrams = sorted(pmi_scores, key=lambda x: x[1], reverse=True)

print(len(sorted_bigrams))

"""#### Filtering out bigrams that have a PMI more than 5

*(I am doing this to simplify the next step of filtering out from these meaningful bigrams, those that actually have collocations)*
"""

filt_sorted_bigrams = list(filter(lambda x: x[1] > 5, sorted_bigrams))

print(len(filt_sorted_bigrams))

# Searching for rows of comments that have the Bigram collocations

# Create a regex pattern for each bigram
patterns = [r"\b{}\s+{}\b".format(re.escape(bigram[0][0]), re.escape(bigram[0][1])) for bigram in filt_sorted_bigrams]

# Combine all patterns into a single pattern
combined_pattern = "|".join(patterns)

# Search for the combined pattern in the 'comment' column
matches = filtered_comments_df['comment'].str.contains(combined_pattern, regex=True)

# Filter out rows with True matches
matched_comments_df = filtered_comments_df[matches]

print(len(matched_comments_df))
print(patterns)

def check_collocation(text, x, y):
    pattern = r"\b{}\s+{}\b".format(re.escape(x), re.escape(y))
    return bool(re.search(pattern, text))

"""#### Filtering out bigrams that actually have collocations"""

top_meaningful_bigrams = []

for meaningful_bigram in filt_sorted_bigrams:
  x = meaningful_bigram[0][0]
  y = meaningful_bigram[0][1]
  check_bool = False
  # print(x)
  for index, row in matched_comments_df.iterrows():
    if check_collocation(row['comment'], x, y):
      check_bool = True
      break

  if check_bool == True:
    top_meaningful_bigrams.append(meaningful_bigram)

print(top_meaningful_bigrams[:10])

print(len(top_meaningful_bigrams))

"""#### Filtering out top 200 bigram-collocations"""

bigram_token_list = []

# Print top 200 meaningful bigrams
print("Top 200 Meaningful Bigrams:")
for bigram, pmi in sorted(top_meaningful_bigrams, key=lambda x: x[1], reverse=True)[:200]:
    print(bigram, pmi)
    bigram_token_list.append(bigram)

# Initialize MWETokenizer
mwetokenizer = MWETokenizer()
bigram_meaningful_tokens = mwetokenizer.tokenize(['_'.join(bigram) for bigram in bigram_token_list])

print(bigram_meaningful_tokens)

def list_to_txt(lst, filename):
    with open(filename, 'w') as f:
        for item in lst:
            f.write(str(item) + '\n')

list_to_txt(bigram_meaningful_tokens, '125_bigrams.txt') # final list of top 200 meaningful bigrams

"""<div class="alert alert-block alert-warning">
    
### Vocabulary List - Combine generated unigram and bigram tokens <a class="anchor" name="write-vocab"></a>
"""

# Extract unique unigram tokens from final_unigram_tokens
unique_unigram_tokens = set(token for tokens in filtered_comments_df['final_unigram_tokens'] for token in tokens)

# Add bigram_meaningful_tokens list to unique_unigram_tokens
unique_unigram_tokens.update(bigram_meaningful_tokens)

# Sort all tokens alphabetically
vocab = sorted(unique_unigram_tokens)

# Display the sorted tokens
print(type(vocab))
print(vocab)
print(len(vocab))

"""### **Outputting to vocab.txt (Output File #2)**"""

# Output sorted tokens to a .txt file
vocab_main = {}
with open('125_vocab.txt', 'w') as file:
    for index, token in enumerate(vocab):
      vocab_main[token] = index
      file.write(f"{token}:{index}\n")

print(vocab_main)

##### Step 4 COMPLETE!

"""<div class="alert alert-block alert-success">
    
## STEP 5 : Sparse Matrix and CountVec <a class="anchor" name="step5"></a>

### Re-tokenizing text based on bigrams
"""

new_mwe = MWETokenizer(bigram_token_list)
filtered_comments_df['re-tokenized_tokens'] = filtered_comments_df['tokens'].apply(lambda x: new_mwe.tokenize(x))

filtered_comments_df.head(100)

# Search for rows where 'taylor_swift' is present in the 're-tokenized_tokens' column
matching_rows = filtered_comments_df[filtered_comments_df['re-tokenized_tokens'].apply(lambda tokens: 'taylor_swift' in tokens)]

# Extract the channelId from the matching rows
matching_channelIds = matching_rows['channelId'].unique()

# Print the channelId(s) that contain 'taylor_swift'
print("ChannelIds containing 'taylor_swift':", matching_channelIds)

"""### Combining all comments under the same channel"""

# Group the DataFrame by 'channelId' and concatenate the token lists
combined_tokens_df = filtered_comments_df.groupby('channelId')['re-tokenized_tokens'].apply(lambda x: sum(x, [])).reset_index()

# Rename the column
combined_tokens_df.columns = ['channelId', 'combined_tokens']

# Display the resulting DataFrame
combined_tokens_df

for index, row in combined_tokens_df.iterrows():
    combined_tokens_df.at[index, 'token_string'] = ' '.join(row['combined_tokens'])

print(combined_tokens_df.head(100))

"""### Generating Sparse Representation (CountVec)"""

# Define a custom tokenizer function
def custom_tokenizer(doc):
    return doc.split()  # Split the document into tokens based on whitespace

# Initialize CountVectorizer with the vocabulary and custom tokenizer
vectorizer = CountVectorizer(vocabulary=vocab_main, analyzer = "word")

# Fit and transform the combined_tokens column
sparse_representation = vectorizer.fit_transform(combined_tokens_df['token_string'])

# Convert the sparse representation to a DataFrame
sparse_df = pd.DataFrame.sparse.from_spmatrix(sparse_representation)

# Set the column names to the vocabulary tokens
sparse_df.columns = vocab

# Concatenate the sparse representation with the channelId column
sparse_df = pd.concat([combined_tokens_df[['channelId']], sparse_df], axis=1)

# Display the resulting DataFrame
print(sparse_df)

# cheking sparse reprsentation example
vocab2 = vectorizer.get_feature_names_out()
for word, count in zip(vocab2, sparse_representation.toarray()[1]):
    if count > 0:
        print (word, ":", count)

"""### **Outputting to countvec.txt (Output File #3)**

"""

# Define the filename for the output text file
output_file = '125_countvec.txt'

# Open the file in write mode
with open(output_file, 'w') as file:
    # Iterate over the rows of the sparse DataFrame
    for index, row in sparse_df.iterrows():
        # Write the channel ID to the file
        file.write(f"{row['channelId']}")

        # Iterate over the columns (token indices) and their frequencies
        for column, frequency in row.items():
          if (column != 'channelId'):
            # Write the token index and its frequency to the file
            if frequency > 0:
              file.write(',')
              file.write(f"{vocab_main[column]}:{frequency}")

        # End the line for the current channel ID
        file.write("\n")

print(f"Sparse representation has been written to {output_file}.")

##### Step 5 DONE! TASK 2 DONE!!!!

"""<div class="alert alert-block alert-success">
    
## 7. Summary <a class="anchor" name="summary"></a>

- I have successfully extracted english comments from the input file of those channels with atleast 15 english comments under them and cleaned them of any emoji characters.

- I have performed text processing on these extracted and cleaned english comments to genearate unigram token list and top 200 meaningful bigram token list.

- I have created a vocabulary from these two generated lists and also found the token-frequencies of occurance of these tokens in the comments of each channel.

---

<div class="alert alert-block alert-success">
    
## 8. References <a class="anchor" name="Ref"></a>

[1] Pandas Dataframe handling, https://www.geeksforgeeks.org/python-pandas-dataframe/

[2] Pandas dataframe.drop_duplicates(), https://www.geeksforgeeks.org/python-pandas-dataframe-drop_duplicates/

[3] pandas.concat(), https://pandas.pydata.org/docs/reference/api/pandas.concat.html

[4] Collections library, https://docs.python.org/3/library/collections.html

[5] NLTK RegexpTokenizer, https://www.nltk.org/api/nltk.tokenize.regexp.html

[6] NLTK MWETokenizer, https://www.nltk.org/api/nltk.tokenize.mwe.html

[7] Bigram PMI Measuring, https://tedboy.github.io/nlps/generated/generated/nltk.BigramAssocMeasures.html

[8] Bigram Collocation Finding, https://tedboy.github.io/nlps/generated/generated/nltk.BigramAssocMeasures.html

[9] Detecting Language, https://www.geeksforgeeks.org/detect-an-unknown-language-using-python/

[10] Regex manipulation, functions and pattern matching, https://docs.python.org/3/howto/regex.html#

[11] PMI Concept, https://en.wikipedia.org/wiki/Pointwise_mutual_information

[12] CountVectorizer, https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html

# ALL DONE
                                                                                ðŸ™‚
"""